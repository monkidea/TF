{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Bee Challenge with RNN\n",
    "\n",
    "- Uses seq2seq model\n",
    "\n",
    "[reference](https://github.com/mikesj-public/rnn_spelling_bee/blob/master/spelling_bee_RNN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**what do we need?**\n",
    "\n",
    "- Dataset : Training and test set (x's and y's)\n",
    "- Phonemes : index to phoneme and phoneme to index dictionaries\n",
    "- Alphabets : index to alphabets and alphabets to index dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "START_LINE = 126\n",
    "END_LINE = 133905\n",
    "FILENAME = 'data/cmudict-0.7b'\n",
    "lines = open(FILENAME, 'r', encoding='utf-8', errors='ignore').read().split('\\n')[START_LINE:END_LINE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BENITES  B EH1 N AY0 T S'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phonemes = []\n",
    "words = []\n",
    "\n",
    "for line in lines:\n",
    "    word, pronounce = line.split('  ')\n",
    "    phonemes.append(pronounce.split(' '))\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WONDERFULNESS\n",
      "['F', 'EY1', 'K', 'ER0', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(words))\n",
    "print(random.choice(phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneme_vocab = set([item for row in phonemes for item in row])\n",
    "phoneme_vocab = ['_'] + sorted(list(phoneme_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneme_vocab_size = len(phoneme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need idx2phoneme and phoneme2idx\n",
    "idx2phoneme = dict(enumerate(phoneme_vocab))\n",
    "phoneme2idx = dict(zip(idx2phoneme.values(), idx2phoneme.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(idx2phoneme[34])\n",
    "print(phoneme2idx['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aphabets\n",
    "idx2alpha = dict(enumerate('_abcdefghijklmnopqrstuvwxyz'))\n",
    "alpha2idx = dict(zip(idx2alpha.values(), idx2alpha.keys()))\n",
    "alpha_vocab_len = len('_abcdefghijklmnopqrstuvwxyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - word to phoneme_index dictionaries\n",
    "# - remember words and phonemes? we are gonna use them here\n",
    "# - lets also remove too small (<5 characters) or too large (>15 characters) words\n",
    "word2phoneme_idx = {}\n",
    "for word, phoneme_list in zip(words, phonemes):\n",
    "    if len(word) > 5 and len(word) < 15 and len(phoneme_list) < 16:\n",
    "        word2phoneme_idx[word.lower()] = [phoneme2idx[phoneme] for phoneme in phoneme_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typically the final stage of preprocessing\n",
    "- convert dataset to numpy arrays filled with indices instead of characters, with padding\n",
    "- split into training, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "updated_words = word2phoneme_idx.keys()\n",
    "dataset_len = len(updated_words)\n",
    "\n",
    "# empty numpy arrays to hold the indices\n",
    "dataX = np.zeros([dataset_len,16])\n",
    "dataX = np.zeros([dataset_len,15])\n",
    "\n",
    "for i,word in enumerate(updated_words):\n",
    "    phoneme_list = word2phoneme_idx[word]\n",
    "    # add items to dataX and dataY with padding\n",
    "    for j, n in \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pairs = np.random.permutation(list(word2phoneme_idx.keys()))\n",
    "\n",
    "input_ = np.zeros((len(pairs), 16))\n",
    "labels_ = np.zeros((len(pairs), 15))\n",
    "\n",
    "for i, k in enumerate(pairs):\n",
    "    v = word2phoneme_idx[k]\n",
    "    k = k + \"_\" * (15 - len(k))\n",
    "    v = v + [0] * (16 - len(v))\n",
    "    \n",
    "    for j, n in enumerate(v):\n",
    "        input_[i][j] = n\n",
    "    for j, letter in enumerate(k):\n",
    "        if letter in alpha2idx:\n",
    "            labels_[i][j] = alpha2idx[letter]\n",
    "        \n",
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "\n",
    "input_test   = input_[:10000]\n",
    "input_val    = input_[10000:20000]\n",
    "input_train  = input_[20000:]\n",
    "labels_test  = labels_[:10000]\n",
    "labels_val   = labels_[10000:20000]\n",
    "labels_train = labels_[20000:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33 11 54 21 38  0  0  0  0  0  0  0  0  0  0  0] [ 7 15 18  4  9  5  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(input_test[124], labels_test[124])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving on to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xseq_len = 16\n",
    "yseq_len = 15\n",
    "batch_size = 128\n",
    "xvocab_size = 70\n",
    "yvocab_size = 28\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fddf04b5f98>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "x_ = [ tf.placeholder(tf.int32, shape=[None,], name='x{}'.format(i)) for i in range(xseq_len)]\n",
    "y_ = [ tf.placeholder(tf.int32, shape=[None,], name='y{}'.format(i)) for i in range(yseq_len)]\n",
    "decoder_inputs = [tf.zeros_like(x_[0], dtype=tf.int32, name = \"GO\")] + y_[:-1]\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(emb_dim),\n",
    "        output_keep_prob=keep_prob)\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*3)\n",
    "\n",
    "\n",
    "with tf.variable_scope('decoder') as scope:\n",
    "    decode_outputs, decode_states = tf.nn.seq2seq.embedding_rnn_seq2seq(x_,decoder_inputs, stacked_lstm,\n",
    "                                        xvocab_size, yvocab_size, emb_dim)\n",
    "    scope.reuse_variables()\n",
    "    # testing\n",
    "    decode_outputs_test, decode_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        x_, decoder_inputs, stacked_lstm, xvocab_size, yvocab_size,emb_dim,\n",
    "        feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we weight the losses based on timestep of decoder output\n",
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in y_] # gives [1, 1, ..., 1,1] - equal weights\n",
    "loss = tf.nn.seq2seq.sequence_loss(decode_outputs, y_, loss_weights, yvocab_size)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.iter)\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = next(iter)\n",
    "        X, Y = zip(*[self.data[i] for i in idxs])\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(list(self.data)), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(list(self.data)))), splits)[:-1]\n",
    "        return iter(it)\n",
    "    \n",
    "train_iter = DataIterator(data_train, 128)\n",
    "val_iter = DataIterator(data_val, 128)\n",
    "test_iter = DataIterator(data_test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(input_seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(output_seq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-32b259deff8a>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-16614357623a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-6e24a7453ba2>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(data_iter)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-32b259deff8a>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_random_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not an iterator"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 16)\n",
    "            print(\"val loss   : {0}, val predict   = {1}\".format(val_loss, val_predict * 100))\n",
    "            print(\"train loss : {0}, train predict = {1}\".format(train_loss, train_predict * 100))\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data, data_utils\n",
    "import importlib as I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/home/suriya/_/tf/TF/simple-seq2seq/data_utils.py'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctl, idx_words, idx_phonemes = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_words, idx_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  5, 12, 20, 26,  5, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_batch_gen = data_utils.batch_gen(testX, testY, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchX, batchY = test_batch_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3, 15, 12, 20, 18,  1,  9, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2,  5, 18,  5, 14, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 3,  8,  5, 13, 12,  1, 23, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 3, 12,  1, 25, 13, 15, 14, 20,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [13, 21,  7, 14,  9, 25,  1,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 4, 21,  3, 11, 23,  1, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 14,  4, 18,  5,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2, 12, 15, 15, 13,  9, 14,  7,  4,  1, 12,  5,  0,  0,  0,  0],\n",
       "        [ 1, 19, 16,  9, 18,  1, 14, 20,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [15, 21, 20, 19, 11,  9, 18, 20, 19,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2, 18, 15,  4, 14,  1, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19, 23,  5, 12, 20, 18, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [23,  8,  9, 19, 20, 12,  5, 18, 19,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [18,  5, 19,  9,  7, 14,  5,  5, 19,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19,  3, 18, 15,  7,  7,  9, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [13,  5, 18,  9, 20, 15,  3, 18,  1,  3, 25,  0,  0,  0,  0,  0],\n",
       "        [19, 16,  5, 14, 19,  5, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5, 24, 16, 18, 15, 16, 18,  9,  1, 20,  9, 15, 14,  0,  0,  0],\n",
       "        [19,  8, 18,  5,  4,  4,  5, 18, 19,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 3,  1, 12, 12,  1,  8,  1, 13,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 9, 13, 16,  5, 18,  1, 20, 15,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 20, 20,  5, 14,  4,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [21, 14, 23,  1, 18, 18,  1, 14, 20,  5,  4,  0,  0,  0,  0,  0],\n",
       "        [18,  5,  9,  3,  8, 12,  9, 14,  7,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [22,  1, 14,  8, 15, 15, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19,  9, 13,  9, 14,  7, 20, 15, 14,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 3,  8, 18, 15, 14, 15, 23,  9, 20, 26,  0,  0,  0,  0,  0,  0],\n",
       "        [ 8,  9, 18,  1, 25,  1, 13,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [16,  1, 19, 19,  1,  2, 12, 25,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [16,  9, 12,  7, 18,  5,  5, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [18,  1, 14,  3,  8, 12,  1, 14,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19,  3,  1, 12, 26,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32),\n",
       " array([[42, 48, 43, 57, 54, 31, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19, 24, 54,  7, 45, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [42, 24, 44, 43, 12, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [42, 43, 30, 44,  3, 45, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [44,  8, 33, 45, 35, 67,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [21,  8, 42, 66, 12, 43,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 45, 21, 54, 30,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19, 43, 63, 44, 35, 46, 21, 31, 43,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5, 55, 53, 26,  7, 45, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [14, 57, 55, 42, 28, 57, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [19, 54,  2, 21, 45,  4, 42, 55,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [55, 66, 24, 43, 57, 54, 38,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [66, 36, 55, 43, 26, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [54, 25, 68, 35, 33, 45, 39, 68,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [55, 42, 54,  2, 33, 35, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [44, 23, 54, 35, 57, 11, 42, 54,  7, 55, 38,  0,  0,  0,  0,  0],\n",
       "        [55, 53, 24, 45, 55, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [25, 42, 55, 53, 54, 49, 53, 54, 38, 30, 56,  7, 45,  0,  0,  0],\n",
       "        [56, 54, 24, 21, 26, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [42,  5, 43,  7, 34,  6, 44,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [37, 44, 53, 26,  2, 57, 47,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 7, 57, 24, 45, 21, 35, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 7, 45, 66, 11, 54,  7, 45, 57, 35, 21,  0,  0,  0,  0,  0,  0],\n",
       "        [54, 17, 42,  7, 43, 35, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [65,  6, 45, 34, 60, 42,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [55, 36, 44, 35, 46, 57,  7, 45,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [42, 54,  2, 45,  7, 66, 35, 57, 55,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [34, 35, 54,  1, 67,  2, 44,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [53,  5, 55,  7, 19, 43, 38,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [53, 35, 43, 33, 54, 39, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [54,  5, 45, 20, 43,  7, 45, 21,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [55, 42,  2, 43, 68, 38,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49470, 16) (10600, 16) (10600, 16)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, testX.shape, validX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feed(X, Y):\n",
    "    feed_dict = {x_[t]: X[t] for t in range(xseq_len)}\n",
    "    feed_dict.update({y_[t]: Y[t] for t in range(yseq_len)})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_gen = data_utils.batch_gen(trainX, trainY, batch_size)\n",
    "batchX, batchY = train_batch_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict = get_feed(batchX, batchY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict[keep_prob] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[0] = 68 is not in [0, 28)\n\t [[Node: decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/embedding_lookup_14 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding/read, _recv_y13_0)]]\nCaused by op 'decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/embedding_lookup_14', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-515b8b6eb428>\", line 13, in <module>\n    xvocab_size, yvocab_size, emb_dim)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 333, in embedding_rnn_seq2seq\n    feed_previous=feed_previous)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 272, in embedding_rnn_decoder\n    loop_function=loop_function)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 139, in rnn_decoder\n    for i, inp in enumerate(decoder_inputs):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 270, in <genexpr>\n    embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 86, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 980, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0] = 68 is not in [0, 28)\n\t [[Node: decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/embedding_lookup_14 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding/read, _recv_y13_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-3b76b030a1bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0] = 68 is not in [0, 28)\n\t [[Node: decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/embedding_lookup_14 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/embedding/read, _recv_y13_0)]]\nCaused by op 'decoder/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/embedding_lookup_14', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-515b8b6eb428>\", line 13, in <module>\n    xvocab_size, yvocab_size, emb_dim)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 333, in embedding_rnn_seq2seq\n    feed_previous=feed_previous)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 272, in embedding_rnn_decoder\n    loop_function=loop_function)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 139, in rnn_decoder\n    for i, inp in enumerate(decoder_inputs):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/seq2seq.py\", line 270, in <genexpr>\n    embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 86, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 980, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    _, out = sess.run([train_op, loss], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
