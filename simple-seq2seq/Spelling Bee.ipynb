{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Bee Challenge with RNN\n",
    "\n",
    "- Uses seq2seq model\n",
    "\n",
    "[reference](https://github.com/mikesj-public/rnn_spelling_bee/blob/master/spelling_bee_RNN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**what do we need?**\n",
    "\n",
    "- Dataset : Training and test set (x's and y's)\n",
    "- Phonemes : index to phoneme and phoneme to index dictionaries\n",
    "- Alphabets : index to alphabets and alphabets to index dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "START_LINE = 126\n",
    "END_LINE = 133905\n",
    "FILENAME = 'data/cmudict-0.7b'\n",
    "lines = open(FILENAME, 'r', encoding='utf-8', errors='ignore').read().split('\\n')[START_LINE:END_LINE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DAMERLIN(1)  D AE1 M ER0 L IH0 N'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phonemes = []\n",
    "words = []\n",
    "\n",
    "for line in lines:\n",
    "    word, pronounce = line.split('  ')\n",
    "    phonemes.append(pronounce.split(' '))\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STITZEL\n",
      "['K', 'AH0', 'D', 'AA1', 'F', 'IY0']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(words))\n",
    "print(random.choice(phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneme_vocab = set([item for row in phonemes for item in row])\n",
    "phoneme_vocab = ['_'] + sorted(list(phoneme_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneme_vocab_size = len(phoneme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need idx2phoneme and phoneme2idx\n",
    "idx2phoneme = dict(enumerate(phoneme_vocab))\n",
    "phoneme2idx = dict(zip(idx2phoneme.values(), idx2phoneme.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(idx2phoneme[34])\n",
    "print(phoneme2idx['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aphabets\n",
    "idx2alpha = dict(enumerate('_abcdefghijklmnopqrstuvwxyz'))\n",
    "alpha2idx = dict(zip(idx2alpha.values(), idx2alpha.keys()))\n",
    "alpha_vocab_len = len('_abcdefghijklmnopqrstuvwxyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - word to phoneme_index dictionaries\n",
    "# - remember words and phonemes? we are gonna use them here\n",
    "# - lets also remove too small (<5 characters) or too large (>15 characters) words\n",
    "word2phoneme_idx = {}\n",
    "for word, phoneme_list in zip(words, phonemes):\n",
    "    if len(word) > 5 and len(word) < 15 and len(phoneme_list) < 16:\n",
    "        word2phoneme_idx[word.lower()] = [phoneme2idx[phoneme] for phoneme in phoneme_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typically the final stage of preprocessing\n",
    "- convert dataset to numpy arrays filled with indices instead of characters, with padding\n",
    "- split into training, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "updated_words = word2phoneme_idx.keys()\n",
    "dataset_len = len(updated_words)\n",
    "\n",
    "# empty numpy arrays to hold the indices\n",
    "dataX = np.zeros([dataset_len,16])\n",
    "dataX = np.zeros([dataset_len,15])\n",
    "\n",
    "for i,word in enumerate(updated_words):\n",
    "    phoneme_list = word2phoneme_idx[word]\n",
    "    # add items to dataX and dataY with padding\n",
    "    for j, n in \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pairs = np.random.permutation(list(word2phoneme_idx.keys()))\n",
    "\n",
    "input_ = np.zeros((len(pairs), 16))\n",
    "labels_ = np.zeros((len(pairs), 15))\n",
    "\n",
    "for i, k in enumerate(pairs):\n",
    "    v = word2phoneme_idx[k]\n",
    "    k = k + \"_\" * (15 - len(k))\n",
    "    v = v + [0] * (16 - len(v))\n",
    "    \n",
    "    for j, n in enumerate(v):\n",
    "        input_[i][j] = n\n",
    "    for j, letter in enumerate(k):\n",
    "        if letter in alpha2idx:\n",
    "            labels_[i][j] = alpha2idx[letter]\n",
    "        \n",
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "\n",
    "input_test   = input_[:10000]\n",
    "input_val    = input_[10000:20000]\n",
    "input_train  = input_[20000:]\n",
    "labels_test  = labels_[:10000]\n",
    "labels_val   = labels_[10000:20000]\n",
    "labels_train = labels_[20000:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68  7 45 24 43  7  0  0  0  0  0  0  0  0  0  0] [26  1 14  5 12 12  1  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(input_test[124], labels_test[124])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving on to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xseq_len = 16\n",
    "yseq_len = 15\n",
    "batch_size = 128\n",
    "xvocab_size = 70\n",
    "yvocab_size = 28\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f916addf898>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "x_ = [ tf.placeholder(tf.int32, shape=[None,], name='x{}'.format(i)) for i in range(xseq_len)]\n",
    "y_ = [ tf.placeholder(tf.int32, shape=[None,], name='y{}'.format(i)) for i in range(yseq_len)]\n",
    "decoder_inputs = [tf.zeros_like(x_[0], dtype=tf.int32, name = \"GO\")] + y_[:-1]\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(emb_dim),\n",
    "        output_keep_prob=keep_prob)\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*3)\n",
    "\n",
    "\n",
    "with tf.variable_scope('decoder') as scope:\n",
    "    decode_outputs, decode_states = tf.nn.seq2seq.embedding_rnn_seq2seq(x_,decoder_inputs, stacked_lstm,\n",
    "                                        xvocab_size, yvocab_size, emb_dim)\n",
    "    scope.reuse_variables()\n",
    "    # testing\n",
    "    decode_outputs_test, decode_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        x_, decoder_inputs, stacked_lstm, xvocab_size, yvocab_size,emb_dim,\n",
    "        feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we weight the losses based on timestep of decoder output\n",
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in y_] # gives [1, 1, ..., 1,1] - equal weights\n",
    "loss = tf.nn.seq2seq.sequence_loss(decode_outputs, y_, loss_weights, yvocab_size)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
