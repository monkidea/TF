{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/home/suriya/_/tf/TF/simple-seq2seq/data_utils.py'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data, data_utils\n",
    "import importlib as I\n",
    "I.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctl, idx_words, idx_phonemes = data.load_data()\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_words, idx_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters \n",
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "batch_size = 128\n",
    "xvocab_size = len(data_ctl['idx2alpha'].keys())  # 27 - 1 = 26\n",
    "yvocab_size = len(data_ctl['idx2pho'].keys())  # 70 - 1 = 69\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_ip = [ tf.placeholder(dtype=tf.int32,\n",
    "                       shape = (None,),\n",
    "                       name = 'ei_{}'.format(i)) for i in range(xseq_len) ]\n",
    "# alternatively\n",
    "#  enc_ip = tf.placeholder(shape=[None,xseq_len], dtype=tf.int32, name='enc_ip')\n",
    "labels = [ tf.placeholder(dtype=tf.int32,\n",
    "                       shape = (None,),\n",
    "                       name = 'ei_{}'.format(i)) for i in range(yseq_len) ]\n",
    "# alternatively\n",
    "#  labels = tf.placeholder(shape=[None,yseq_len], dtype=tf.int32, name='labels')\n",
    "dec_ip = [ tf.zeros_like(enc_ip[0], dtype=tf.int32, name='GO')] + labels[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(emb_dim, state_is_tuple=True),\n",
    "        output_keep_prob=keep_prob)\n",
    "stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*3, state_is_tuple=True)\n",
    "\n",
    "\n",
    "with tf.variable_scope('decoder') as scope:\n",
    "    decode_outputs, decode_states = tf.nn.seq2seq.embedding_rnn_seq2seq(enc_ip,dec_ip, stacked_lstm,\n",
    "                                        xvocab_size, yvocab_size, emb_dim)\n",
    "    scope.reuse_variables()\n",
    "    # testing\n",
    "    decode_outputs_test, decode_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "        enc_ip, dec_ip, stacked_lstm, xvocab_size, yvocab_size,emb_dim,\n",
    "        feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we weight the losses based on timestep of decoder output\n",
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels] # gives [1, 1, ..., 1,1] - equal weights\n",
    "loss = tf.nn.seq2seq.sequence_loss(decode_outputs, labels, loss_weights, yvocab_size)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feed(X, Y):\n",
    "    feed_dict = {enc_ip[t]: X[t] for t in range(xseq_len)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(yseq_len)})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # create a generator\n",
    "    train_batch_gen = data_utils.batch_gen(trainX, trainY, batch_size)\n",
    "    X, Y = train_batch_gen.__next__()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, out = sess.run([train_op, loss], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [19  3 18 15 12 12 19  0  0  0  0  0  0  0  0  0] [55 42 54 48 43 68  0  0  0  0  0  0  0  0  0  0]\n",
      "1 [ 7 18  2  1 22  9  3  1  0  0  0  0  0  0  0  0] [33 26 19  2 65 35 42  7  0  0  0  0  0  0  0  0]\n",
      "2 [ 8 25  4 18  1 20  5  4  0  0  0  0  0  0  0  0] [34 17 21 54 31 57  7 21  0  0  0  0  0  0  0  0]\n",
      "3 [16 18  9 14 20 15 21 20 19  0  0  0  0  0  0  0] [53 54 36 45 57 15 57 55  0  0  0  0  0  0  0  0]\n",
      "4 [20 18  1 14 19 16 15 18 20  5 18 19  0  0  0  0] [57 54  4 45 55 53 11 54 57 26 68  0  0  0  0  0]\n",
      "5 [19  3  8 21  3  8  1 18  4 20  0  0  0  0  0  0] [56  8 42  1 54 57  0  0  0  0  0  0  0  0  0  0]\n",
      "6 [16 18  5 19  3 18  9 16 20  9 15 14 19  0  0  0] [53 54  7 55 42 54 36 53 56  7 45 68  0  0  0  0]\n",
      "7 [19 20  9 16 21 12  1 20  9 15 14  0  0  0  0  0] [55 57 37 53 67  7 43 30 56  7 45  0  0  0  0  0]\n",
      "8 [18  5 24 18 15  4 20  0  0  0  0  0  0  0  0  0] [54 24 42 55 54 47 21  0  0  0  0  0  0  0  0  0]\n",
      "9 [ 9 13  1  7  9 14  5  4  0  0  0  0  0  0  0  0] [37 44  5 41  7 45 21  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "rtest_batch_gen = data_utils.rand_batch_gen(testX, testY, batch_size)\n",
    "for i in range(10):\n",
    "    batchX, batchY = rtest_batch_gen.__next__()\n",
    "    print(i,batchX[40], batchY[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pensioners'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.decode_word(batchX[12], data_ctl['idx2alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P EH1 N SH AH0 N ER0 Z'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.decode_phonemes(batchY[12], data_ctl['idx2pho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(train_batch_gen):\n",
    "    # get batches\n",
    "    batchX, batchY = train_batch_gen.__next__()\n",
    "    # build feed\n",
    "    feed_dict = get_feed(batchX, batchY)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, loss_v = sess.run([train_op, loss], feed_dict)\n",
    "    return loss_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_step(eval_batch_gen):\n",
    "    # get batches\n",
    "    batchX, batchY = eval_batch_gen.__next__()\n",
    "    # build feed\n",
    "    feed_dict = get_feed(batchX, batchY)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    loss_v, dec_op_v = sess.run([loss, decode_outputs_test], feed_dict)\n",
    "    # dec_op_v is a list; also need to transpose 0,1 indices\n",
    "    dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "    return loss_v, dec_op_v, batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_batch(eval_batch_gen, num_batches):\n",
    "    losses, predict_loss = [], []\n",
    "    for i in range(num_batches):\n",
    "        loss_v, dec_op_v, batchX, batchY = eval_step(eval_batch_gen)\n",
    "        losses.append(loss_v)\n",
    "        for j in range(len(dec_op_v)):\n",
    "            real = batchX.T[j]\n",
    "            predict = np.argmax(dec_op_v, axis=2)[j]\n",
    "            predict_loss.append(all(real == predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    batchX, batchY = train_batch_gen.__next__()\n",
    "    feed_dict = get_feed(batchX, batchY)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    loss_v, dec_op_val = sess.run([loss, decode_outputs_test], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 70)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_op_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 16) (128, 16)\n"
     ]
    }
   ],
   "source": [
    "#train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n",
    "a,b = train_batch_gen.__next__()\n",
    "print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss   : 4.202735900878906, val predict   = 0.0%\n",
      "train loss : 4.201011657714844, train predict = 0.0%\n"
     ]
    }
   ],
   "source": [
    "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 16)\n",
    "train_eval_batch_gen = data_utils.rand_batch_gen(trainX, trainY, 16)\n",
    "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, 128)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(100000):\n",
    "    try:\n",
    "        train_batch(train_batch_gen)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_batch_gen, 16)\n",
    "            train_loss, train_predict = eval_batch(train_eval_batch_gen, 16)\n",
    "            print(\"val loss   : {0}, val predict   = {1}%\".format(val_loss, val_predict * 100))\n",
    "            print(\"train loss : {0}, train predict = {1}%\".format(train_loss, train_predict * 100))\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted by user\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
