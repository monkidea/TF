{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To classify images using a reccurent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "training_steps = 100000# 1 million\n",
    "batch_size = 128\n",
    "display_step = 100 # print model's accuracy every \"display_step\" steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "timesteps = 28 # 28x28\n",
    "num_hidden = 128 # number of hidden units\n",
    "num_classes = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.placeholder(dtype, shape=None, name=None)\n",
    "x = tf.placeholder(tf.float32, shape=[None,timesteps,input_dim], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name = 'Y')\n",
    "# initial state of cell\n",
    "istate = tf.placeholder(tf.float32, [None, 2*num_hidden])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden' : tf.Variable(tf.random_normal([input_dim, num_hidden])),\n",
    "    'out' : tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden' : tf.Variable(tf.random_normal([num_hidden])),\n",
    "    'out' : tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(_X, istate, weights, biases ): # input to RNN : X, initial state of cell, weights and biases\n",
    "    # input shape: (batch_size, timesteps, input_dim)\n",
    "    # permute timesteps and batch_size\n",
    "    _X = tf.transpose(_X, [1,0,2])\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, input_dim])\n",
    "    # activation\n",
    "    _activation = tf.matmul(_X, weights['hidden']) + biases['hidden']\n",
    "    # create a cell\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=num_hidden)\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _activations = tf.split(0,timesteps, _activation) # split along 0th dimenstion, into \"timesteps\" items\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.rnn(cell=lstm_cell, inputs=_activations, initial_state=istate)\n",
    "    \n",
    "    # output activation\n",
    "    return tf.matmul(outputs[-1],weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model, Training ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f156d589ed0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "# Ouput prediction\n",
    "pred = RNN(x,istate,weights,biases)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # get average of all softmax losses\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "correct_pred = tf.equal( tf.argmax(y,1), tf.argmax(pred,1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 100\t Batch Loss : 0.690336465836\t Accuracy : 0.8046875\n",
      "Iteration : 200\t Batch Loss : 0.362896382809\t Accuracy : 0.8671875\n",
      "Iteration : 300\t Batch Loss : 0.226416990161\t Accuracy : 0.90625\n",
      "Iteration : 400\t Batch Loss : 0.119445301592\t Accuracy : 0.9765625\n",
      "Iteration : 500\t Batch Loss : 0.171115219593\t Accuracy : 0.9296875\n",
      "Iteration : 600\t Batch Loss : 0.192652314901\t Accuracy : 0.9296875\n",
      "Iteration : 700\t Batch Loss : 0.168055459857\t Accuracy : 0.9296875\n",
      ">> Optimization complete\n",
      ">> Final Accuracy : 0.96875\n"
     ]
    }
   ],
   "source": [
    "# init all variables\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    step = 1\n",
    "    # run training op for \"training_steps\" num of times\n",
    "    while batch_size*step < training_steps:\n",
    "        # get training batches\n",
    "        batchX, batchY = mnist.train.next_batch(batch_size)\n",
    "        # reshape batchX to 3D tensor : [ batch_size x timesteps x input_dim ]\n",
    "        batchX = batchX.reshape([batch_size,timesteps, input_dim])\n",
    "        # run training op\n",
    "        sess.run(optimizer, feed_dict={\n",
    "                x : batchX,\n",
    "                y : batchY,\n",
    "                istate : np.zeros([batch_size, 2*num_hidden])\n",
    "            })\n",
    "        # evaluate model every \"display_step\" times\n",
    "        if not step % display_step:\n",
    "            # accuracy\n",
    "            # get test batches\n",
    "            t_batchX, t_batchY = mnist.test.next_batch(batch_size)\n",
    "            t_batchX = t_batchX.reshape([batch_size,timesteps, input_dim])\n",
    "            acc, loss_val = sess.run([accuracy,loss], feed_dict = {\n",
    "                    x : t_batchX,\n",
    "                    y : t_batchY,\n",
    "                    istate : np.zeros([batch_size, 2*num_hidden])\n",
    "                })\n",
    "            print('Iteration : {0}\\t Batch Loss : {1}\\t Accuracy : {2}'.format(step,loss_val,acc))\n",
    "        step += 1\n",
    "        \n",
    "    # out of the loop\n",
    "    print('>> Optimization complete')\n",
    "    # get final accuracy\n",
    "    t_batchX, t_batchY = mnist.test.next_batch(batch_size*2)\n",
    "    t_batchX = t_batchX.reshape([batch_size*2,timesteps, input_dim])\n",
    "    acc = sess.run(accuracy, feed_dict= {\n",
    "            x : t_batchX,\n",
    "            y : t_batchY,\n",
    "            istate : np.zeros([2*batch_size,2*num_hidden])\n",
    "        })\n",
    "    print('>> Final Accuracy : {}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
